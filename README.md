# CEP_IITB_NRSC_2024
Basic coding for DeepLearning using Google colab
### Day 1 (04/03/24) : Coding (Basics of Pytorch, MLP)
In this coding tutorial, we delve into the fundamentals of PyTorch, a powerful open-source machine learning library, and explore the construction and training of Multi-Layer Perceptrons (MLPs). Multi-Layer Perceptrons are the building blocks of many deep learning models and serve as an excellent starting point for understanding neural networks.

Throughout the tutorial, we will cover essential concepts such as defining neural network architectures, implementing forward and backward propagation, optimizing model parameters using gradient descent, and evaluating model performance. We will leverage PyTorch's intuitive syntax and computational graph capabilities to create and train MLPs efficiently.

The tutorial will start with a brief overview of PyTorch, including tensor operations, automatic differentiation, and GPU acceleration, to provide a solid foundation for understanding the subsequent concepts. We will then proceed to implement a simple MLP architecture using PyTorch's nn.Module class, composing multiple fully connected layers with non-linear activation functions.

Moreover, we will discuss the importance of choosing appropriate activation functions, initializing model parameters effectively, and selecting loss functions and optimization algorithms suited for training MLPs. Practical examples and demonstrations will be provided to illustrate these concepts in action.

By the end of this tutorial, participants will gain a comprehensive understanding of PyTorch basics and be equipped with the knowledge and skills to construct, train, and evaluate MLP models for various machine learning tasks.
### Day 2 (05/03/24) : Coding (CNN for classification)
Convolutional Neural Networks (CNNs) are a class of deep neural networks specifically designed for tasks involving image recognition and classification. Comprising multiple layers, including convolutional, pooling, and fully connected layers, CNNs excel at automatically learning and extracting intricate patterns and features from input images. Their hierarchical architecture allows for effective feature representation, enabling the network to discern complex relationships within images and make accurate predictions. Through convolutional operations, CNNs capture local spatial dependencies, while pooling layers reduce dimensionality and retain important features. Fully connected layers at the end of the network aggregate extracted features to generate class predictions. CNNs have revolutionized computer vision tasks, achieving state-of-the-art performance in image classification, object detection, and semantic segmentation, making them a cornerstone of modern machine learning and artificial intelligence applications. In this tutorial, we will delve into the fundamentals of CNNs, exploring their architecture, training process, and applications, equipping you with the knowledge and skills to leverage CNNs effectively for image classification tasks.
### Day 3 (06/03/24) : Coding (pre-trained convolutional neural networks (CNNs))
Land classification using pre-trained convolutional neural networks (CNNs) involves leveraging deep learning models that have been trained on large-scale datasets, such as ImageNet, for the task of classifying land cover and land use categories from satellite or aerial imagery. These pre-trained CNNs have learned to extract meaningful features from images through millions of iterations, capturing rich spatial and spectral information. By fine-tuning these pre-trained models on the target land classification dataset, we can adapt them to recognize specific land cover classes. Fine-tuning involves updating the weights of the pre-trained model using the target dataset, which typically requires fewer annotated samples compared to training from scratch. This approach accelerates model training and often leads to better generalization performance. Leveraging pre-trained CNNs for land classification allows us to harness the power of deep learning in extracting intricate patterns and features from imagery, enabling accurate and efficient classification of land cover and land use types.
### Day 4 (07/03/24) : Coding (ViT, Yolo)
The first part focused on ViT, a cutting-edge model architecture that applies the Transformer model to images. The code demonstrated how to leverage a pre-trained ViT model using the Hugging Face `transformers` library. After loading the pre-trained model and its feature extractor, an image was preprocessed and fed into the model for inference. The output provided predictions regarding the class of the input image.

Moving on to YOLO, an efficient object detection algorithm, the code illustrated how to use the YOLO library to detect objects in an image. Following the initialization of the YOLO detector, an image was loaded and passed through the detector for inference. The results were then processed to identify detected objects along with their corresponding bounding boxes.

### Day 5 (08/03/24) : Coding (GAN)
Generative Adversarial Networks (GANs) are employed to generate synthetic images with remarkable realism. By crafting a GAN architecture comprising a generator and discriminator, and training it on a dataset of real images, the generator learns to produce compelling synthetic images while the discriminator refines its ability to differentiate real from fake. Through iterative training, involving optimization of loss functions and architectural modifications, the GAN evolves to generate diverse and high-fidelity images, suitable for applications ranging from data augmentation to creative artwork. Evaluation encompasses both quantitative metrics like Inception Score (IS) and qualitative assessment through visual inspection, empowering the exploration and customization of GANs for diverse image generation tasks.
